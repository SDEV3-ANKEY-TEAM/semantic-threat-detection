# llm/predictor.py
import os
from openai import OpenAI
from dotenv import load_dotenv
from llm.prompt_template import build_prompt

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def query_gpt(prompt: str, model: str = "gpt-4o") -> str:
    """
    OpenAI GPT ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  ì‘ë‹µ ë°›ê¸° (v1.x SDK í˜¸í™˜)
    """
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a cybersecurity analyst."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
            max_tokens=300,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[ERROR] {str(e)}"


# ì˜ˆì‹œ ì‹¤í–‰
def example():
    trace_id = "trace-001"
    sequence = "powershell.exe ì‹¤í–‰ â†’ ì¸ì½”ë”©ëœ PowerShell ëª…ë ¹ì–´ ì‹¤í–‰ â†’ evil.ps1 íŒŒì¼ ìƒì„± ë˜ëŠ” ìˆ˜ì • â†’ ì™¸ë¶€ ì ‘ì† ì‹œë„: 185.42.99.12:443"
    sigma = "Encoded PowerShell, Suspicious File Creation"
    prompt = build_prompt(trace_id, sequence, sigma)
    result = query_gpt(prompt)
    print("ğŸ” GPT ì‘ë‹µ:")
    print(result)


if __name__ == "__main__":
    example()
